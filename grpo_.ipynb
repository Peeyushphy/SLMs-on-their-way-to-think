{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "126610d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "import importlib\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from rich import print\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0c7e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3523b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('/Users/peeyushsharma/personal/reasoning_slm/lora_sft', device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba75166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('HuggingFaceTB/SmolLM2-360M-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ada68a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import reasoning_gym\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def load_model(model_id, device):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                 dtype=torch.bfloat16,\n",
    "                                                 device_map=device)\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "    \n",
    "    \n",
    "    full_responses = []\n",
    "    for sample in batch:\n",
    "        question = sample['question']\n",
    "        answer = f\"<think>....</think> <answer>{sample['answer']}</answer>.\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
    "The assistant first thinks about the reasoning process in the mind and then provides the user\n",
    "with the answer. The reasoning process and answer are enclosed within <think> </think> and\n",
    "<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think>\n",
    "<answer> answer here </answer>. User: {question}. Assistant:\"\"\"\n",
    "\n",
    "        full_response = prompt + \" \" + answer + tokenizer.eos_token\n",
    "        \n",
    "        full_responses.append(full_response)\n",
    "    \n",
    "    input_tokenized = tokenizer(full_responses, padding=True, add_special_tokens=False, return_tensors='pt')['input_ids']\n",
    "    \n",
    "    labels_tokenized = tokenizer([\" \" + f\"<think>....</think> <answer>{sample['answer']}</answer>.\" + tokenizer.eos_token for sample in batch],\n",
    "                                 add_special_tokens=False,\n",
    "                                 return_tensors='pt',\n",
    "                                 padding='max_length',\n",
    "                                 max_length=input_tokenized.shape[1]\n",
    "                                 )['input_ids']\n",
    "    \n",
    "    labels_tokenized = torch.where(labels_tokenized != tokenizer.pad_token_id, labels_tokenized, -100)\n",
    "    labels_tokenized[:, -1] = tokenizer.pad_token_id\n",
    "    \n",
    "    input_ids_tokenized_left_shifted = input_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_tokenized[:, 1:]\n",
    "    \n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input\" : input_ids_tokenized_left_shifted,\n",
    "        \"target\" : labels_tokenized_right_shifted,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    \n",
    "def get_dataloader(data_set, tokenizer):\n",
    "    gym_data = reasoning_gym.create_dataset(data_set, size=50, seed=42)\n",
    "    data = dataset(gym_data)\n",
    "    collate = partial(collate_fn, tokenizer=tokenizer)\n",
    "    return DataLoader(\n",
    "        data,\n",
    "        batch_size=8,\n",
    "        collate_fn=collate,\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d5223f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves\n",
    "it. The assistant first thinks about the reasoning process in the mind and then provides the user\n",
    "with the answer. The reasoning process and answer are enclosed within <think>...</think>and <answer>...</answer> tags, respectively, i.e., <think> reasoning process here </think>\n",
    "<answer> answer here </answer>. User: How are you model?. Assistan\"\"\"\n",
    "\n",
    "tokens = tokenizer(prompt, add_special_tokens=False, return_tensors='pt')['input_ids']\n",
    "out = model(tokens, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "686ff010",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_logits = out['logits'].argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d219b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basicenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
