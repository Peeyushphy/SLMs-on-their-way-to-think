{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "126610d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "import importlib\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from rich import print\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import reasoning_gym\n",
    "from reasoning_gym import get_score_answer_fn\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "96c12897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "FORMAT_REWARD_WEIGHT = 0.15\n",
    "CORRECTNESS_REWARD_WEIGHT = 0.85\n",
    "MAX_NEW_TOKENS = 150\n",
    "N_ROLLOUTS = 3\n",
    "BATCH_SIZE = 2\n",
    "BUFFER_SIZE = 15\n",
    "EPOCHS = 2  # Reduced for demo\n",
    "LR = 1e-5   # Lower LR is usually better for PPO/GRPO\n",
    "\n",
    "# Initialize Accelerator\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "efd55aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(answer):\n",
    "    match = re.search(r\"<answer>(.*?)</answer>\", answer, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return answer\n",
    "\n",
    "def calculate_format_reward(response):\n",
    "    # required tags\n",
    "    required = [\"<think>\", \"</think>\", \"<answer>\", \"</answer>\"]\n",
    "    if any(tag not in response for tag in required):\n",
    "        return -0.5\n",
    "\n",
    "    think_open = response.find(\"<think>\")\n",
    "    think_close = response.find(\"</think>\")\n",
    "    answer_open = response.find(\"<answer>\")\n",
    "    answer_close = response.find(\"</answer>\")\n",
    "\n",
    "    # enforce correct order\n",
    "    if not (0 <= think_open < think_close < answer_open < answer_close):\n",
    "        return -0.5\n",
    "\n",
    "    reward = 0.0\n",
    "    reward += 0.2  # correct format\n",
    "    reward += 0.3  # answer block exists\n",
    "    return reward\n",
    "\n",
    "def correctness_reward(response, validation_object):\n",
    "    # This might fail if the dataset requires specific logic, wrapping in try/except is safer\n",
    "    try:\n",
    "        score_fn = get_score_answer_fn(validation_object['metadata']['source_dataset'])\n",
    "        return score_fn(response, validation_object)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def calculate_rewards(batch_response, validation_objects):\n",
    "    format_r = np.array([calculate_format_reward(r) for r in batch_response])\n",
    "    correctness_r = np.array([\n",
    "        correctness_reward(extract_answer(r), val_obj) \n",
    "        for val_obj, r in zip(validation_objects, batch_response)\n",
    "    ])\n",
    "    return FORMAT_REWARD_WEIGHT * format_r + CORRECTNESS_REWARD_WEIGHT * correctness_r\n",
    "\n",
    "# --- Model & Math Utils ---\n",
    "\n",
    "def calculate_logits(model, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Calculates token-level log probabilities for the specific tokens in input_ids.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits[:, :-1, :]  # Shift so we predict next token\n",
    "        input_ids_shifted = input_ids[:, 1:]\n",
    "        \n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        token_logprobs = torch.gather(log_probs, 2, input_ids_shifted.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        return token_logprobs\n",
    "\n",
    "def left_pad(sequences, pad_value):\n",
    "    \"\"\"Left-pads a list of 1D tensors.\"\"\"\n",
    "    max_len = max(len(s) for s in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    dtype = sequences[0].dtype\n",
    "    device = sequences[0].device\n",
    "    \n",
    "    padded_batch = torch.full((batch_size, max_len), fill_value=pad_value, dtype=dtype, device=device)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_batch[i, -len(seq):] = seq\n",
    "        \n",
    "    return padded_batch\n",
    "\n",
    "# --- Data Loading ---\n",
    "\n",
    "class GymDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "    prompts = []\n",
    "    for sample in batch:\n",
    "        question = sample['question']\n",
    "        prompt = (\n",
    "            f\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\\n\"\n",
    "            f\"The assistant first thinks about the reasoning process in the mind and then provides the user \"\n",
    "            f\"with the answer.\\nUser: {question}.\\nAssistant:\"\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    # Basic tokenization of prompts\n",
    "    input_tokenized = tokenizer(prompts, padding=True, add_special_tokens=False, return_tensors='pt')\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_tokenized['input_ids'],\n",
    "        'attention_mask': input_tokenized['attention_mask'],\n",
    "        'raw_batch': batch  # Keep original objects for validation\n",
    "    }\n",
    "\n",
    "def get_dataloader(dataset_name, tokenizer, batch_size=2):\n",
    "    # Note: 'syllogism' might need to be 'logic/syllogism' or similar depending on reasoning_gym version\n",
    "    gym_data = reasoning_gym.create_dataset(dataset_name, seed=42, size=100) \n",
    "    data = GymDataset(gym_data)\n",
    "    return DataLoader(\n",
    "        data,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=partial(collate_fn, tokenizer=tokenizer),\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c33eb884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_exp(model, tokenizer, input_ids, raw_batch, attention_mask):\n",
    "    model.eval() # Ensure eval mode for generation\n",
    "    \n",
    "    # 1. Generate Rollouts\n",
    "    with torch.no_grad():\n",
    "        # input_ids shape: [B, Seq]\n",
    "        # full_response shape: [B * N_ROLLOUTS, Total_Seq]\n",
    "        full_response = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            temperature=1.0,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=N_ROLLOUTS,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        # 2. Separate Prompt and Completion\n",
    "        # Note: full_response includes input_ids. We need to find where generation started.\n",
    "        prompt_len = input_ids.shape[1]\n",
    "        completion_ids = full_response[:, prompt_len:]\n",
    "        \n",
    "        # 3. Calculate Log Probs of the Generated Sequence (Reference Policy)\n",
    "        # We need full attention mask for the generated sequence\n",
    "        full_attention_mask = (full_response != tokenizer.pad_token_id).long()\n",
    "        \n",
    "        # This returns log_probs for tokens at indices [1:] (next token prediction)\n",
    "        # Shape: [B*N, Total_Seq-1]\n",
    "        all_token_log_probs = calculate_logits(model, full_response, full_attention_mask)\n",
    "        \n",
    "        # Extract only the completion log probs\n",
    "        # We align by slicing. all_token_log_probs index i corresponds to prediction of full_response[i+1]\n",
    "        # The first token of completion is at prompt_len. It was predicted by token at prompt_len-1.\n",
    "        completion_log_probs = all_token_log_probs[:, prompt_len-1:]\n",
    "\n",
    "        # 4. Decoding & Rewards\n",
    "        decoded_responses = tokenizer.batch_decode(completion_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Repeat validation objects for the rollouts\n",
    "        # raw_batch is size B, we need B*N\n",
    "        expanded_val_objs = []\n",
    "        for obj in raw_batch:\n",
    "            expanded_val_objs.extend([obj] * N_ROLLOUTS)\n",
    "\n",
    "        rewards = calculate_rewards(decoded_responses, expanded_val_objs)\n",
    "        \n",
    "        # 5. Advantage Calculation (Group Relative)\n",
    "        # Reshape to [B, N] to normalize within the group\n",
    "        rewards_reshaped = rewards.reshape(-1, N_ROLLOUTS)\n",
    "        mean_rewards = rewards_reshaped.mean(axis=1, keepdims=True)\n",
    "        std_rewards = rewards_reshaped.std(axis=1, keepdims=True) + 1e-8\n",
    "        advantages = (rewards_reshaped - mean_rewards) / std_rewards\n",
    "        \n",
    "        # Flatten back to [B*N] and move to tensor\n",
    "        advantages = torch.tensor(advantages.flatten(), dtype=torch.float32, device=accelerator.device)\n",
    "\n",
    "        experiences = []\n",
    "        for i in range(len(full_response)):\n",
    "            # Create a mask for ONLY the completion part (ignoring padding)\n",
    "            # 1 for valid completion tokens, 0 for everything else\n",
    "            seq_len = full_response[i].size(0)\n",
    "            mask = torch.zeros(seq_len, dtype=torch.float32, device=accelerator.device)\n",
    "            \n",
    "            # Find end of valid tokens (first pad after prompt) or end of seq\n",
    "            is_pad = (full_response[i] == tokenizer.pad_token_id)\n",
    "            # We only care about padding occurring AFTER the prompt\n",
    "            completion_pads = is_pad[prompt_len:]\n",
    "            if completion_pads.any():\n",
    "                # First pad index relative to start of completion\n",
    "                valid_len = completion_pads.nonzero(as_tuple=True)[0][0].item()\n",
    "                end_idx = prompt_len + valid_len\n",
    "            else:\n",
    "                end_idx = seq_len\n",
    "                \n",
    "            mask[prompt_len:end_idx] = 1.0\n",
    "\n",
    "            # Store the log probs needed for optimization (shifted internally in loss calc, but here we store raw)\n",
    "            # Actually, to save memory, we usually just recompute logits during training or store current log_probs.\n",
    "            # Storing `completion_log_probs` requires alignment. \n",
    "            # Let's store the Full Sequence and recompute/extract in training loop for simplicity or alignment.\n",
    "            \n",
    "            experiences.append({\n",
    "                'input_ids': full_response[i], # [Seq]\n",
    "                'attention_mask': full_attention_mask[i], # [Seq]\n",
    "                'response_mask': mask, # [Seq] (1s on completion, 0s elsewhere)\n",
    "                'old_log_probs': all_token_log_probs[i], # [Seq-1]\n",
    "                'advantages': advantages[i] # Scalar\n",
    "            })\n",
    "            \n",
    "    return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6250b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grpo_loss(new_log_probs, old_log_probs, response_mask, advantages):\n",
    "    \"\"\"\n",
    "    new_log_probs: [B, Seq-1]\n",
    "    old_log_probs: [B, Seq-1]\n",
    "    response_mask: [B, Seq] -> Needs to be shifted to [B, Seq-1] to match logits\n",
    "    advantages: [B, 1]\n",
    "    \"\"\"\n",
    "    # Align mask with logits (drop the first token position as logits predict next token)\n",
    "    mask = response_mask[:, 1:]\n",
    "    \n",
    "    # Probability Ratio\n",
    "    # exp(new - old) = new_prob / old_prob\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    \n",
    "    # Clip\n",
    "    clip_epsilon = 0.2\n",
    "    clipped_ratio = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon)\n",
    "    \n",
    "    # Element-wise loss\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = clipped_ratio * advantages\n",
    "    min_surr = torch.min(surr1, surr2)\n",
    "    \n",
    "    # Mask out non-response tokens\n",
    "    masked_loss = min_surr * mask\n",
    "    \n",
    "    # Average over valid response tokens only\n",
    "    loss = -masked_loss.sum() / (mask.sum() + 1e-8)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_step(model, batch, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    # 1. Pad Batch\n",
    "    input_ids = left_pad([b['input_ids'] for b in batch], tokenizer.pad_token_id)\n",
    "    attention_mask = left_pad([b['attention_mask'] for b in batch], 0)\n",
    "    response_mask = left_pad([b['response_mask'] for b in batch], 0)\n",
    "    \n",
    "    # Old log probs need to be padded. They are usually float. Pad with 0.\n",
    "    old_log_probs = left_pad([b['old_log_probs'] for b in batch], 0.0)\n",
    "    \n",
    "    advantages = torch.stack([b['advantages'] for b in batch]).unsqueeze(-1) # [B, 1]\n",
    "    \n",
    "    # 2. Forward Pass (Recompute Logits)\n",
    "    # This calls model() so gradients flow\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits[:, :-1, :]\n",
    "    input_ids_shifted = input_ids[:, 1:]\n",
    "    \n",
    "    log_probs_all = torch.log_softmax(logits, dim=-1)\n",
    "    new_token_log_probs = torch.gather(log_probs_all, 2, input_ids_shifted.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # 3. Calculate Loss\n",
    "    loss = calculate_grpo_loss(\n",
    "        new_log_probs=new_token_log_probs,\n",
    "        old_log_probs=old_log_probs,\n",
    "        response_mask=response_mask,\n",
    "        advantages=advantages\n",
    "    )\n",
    "    \n",
    "    # 4. Backward\n",
    "    optimizer.zero_grad()\n",
    "    accelerator.backward(loss)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1500fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, buffer, optimizer, num_epochs=2):\n",
    "    random.shuffle(buffer)\n",
    "    losses = []\n",
    "    \n",
    "    # TQDM for training epochs\n",
    "    pbar = tqdm(range(num_epochs), desc=\"Training Epochs\", leave=False)\n",
    "    for _ in pbar:\n",
    "        epoch_losses = []\n",
    "        for i in range(0, len(buffer), BATCH_SIZE):\n",
    "            batch = buffer[i : i + BATCH_SIZE]\n",
    "            if len(batch) == 0: continue\n",
    "            \n",
    "            loss_val = train_step(model, batch, optimizer)\n",
    "            epoch_losses.append(loss_val)\n",
    "            \n",
    "        avg_loss = np.mean(epoch_losses) if epoch_losses else 0\n",
    "        losses.append(avg_loss)\n",
    "        pbar.set_postfix({\"Avg Loss\": f\"{avg_loss:.4f}\"})\n",
    "        \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    base_model_id = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    tokenizer.padding_side = \"left\" # Important for generation\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    # Load Model\n",
    "    print(\"[bold green]Loading Model...[/bold green]\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id, \n",
    "        torch_dtype=torch.bfloat16,\n",
    "        # device_map=\"auto\" # Let Accelerator handle device\n",
    "    )\n",
    "    \n",
    "    # Load PEFT adapter (Update path as needed)\n",
    "    model = PeftModel.from_pretrained(base_model, '/Users/peeyushsharma/Downloads/lora_sft_2')\n",
    "    # For demonstration, we just use the base model as if it were the peft model\n",
    "    model = base_model \n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    \n",
    "    # Data Loader\n",
    "    data_loader = get_dataloader('syllogism', tokenizer)\n",
    "    \n",
    "    # Prepare with Accelerator\n",
    "    # Note: If using PeftModel, ensure only adapter params are optimized\n",
    "    model, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)\n",
    "    \n",
    "    buffer = []\n",
    "    \n",
    "    # Main Loop\n",
    "    print(\"[bold green]Starting Collection & Training...[/bold green]\")\n",
    "    \n",
    "    # TQDM over the dataloader\n",
    "    main_pbar = tqdm(data_loader, desc=\"Data Collection\", disable=not accelerator.is_local_main_process)\n",
    "    \n",
    "    for i, sample in enumerate(main_pbar):\n",
    "        # Collect Experience\n",
    "        experiences = collect_exp(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            sample['input_ids'], \n",
    "            sample['raw_batch'], \n",
    "            sample['attention_mask']\n",
    "        )\n",
    "        buffer.extend(experiences)\n",
    "        \n",
    "        # Trigger training when buffer is full\n",
    "        if len(buffer) >= BUFFER_SIZE:\n",
    "            main_pbar.set_description(\"Training...\")\n",
    "            avg_loss = train_loop(model, buffer, optimizer, num_epochs=EPOCHS)\n",
    "            \n",
    "            # Clear buffer and update stats\n",
    "            buffer = []\n",
    "            main_pbar.set_description(f\"Collecting (Last Loss: {avg_loss:.4f})\")\n",
    "            \n",
    "    print(\"[bold green]Finished![/bold green]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basicenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
