{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "126610d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "import importlib\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from rich import print\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import reasoning_gym\n",
    "from reasoning_gym import get_score_answer_fn\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "96c12897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "FORMAT_REWARD_WEIGHT = 0.15\n",
    "CORRECTNESS_REWARD_WEIGHT = 0.85\n",
    "MAX_NEW_TOKENS = 150\n",
    "N_ROLLOUTS = 3\n",
    "BATCH_SIZE = 2\n",
    "BUFFER_SIZE = 15\n",
    "EPOCHS = 2  # Reduced for demo\n",
    "LR = 1e-5   # Lower LR is usually better for PPO/GRPO\n",
    "\n",
    "# Initialize Accelerator\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "efd55aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(answer):\n",
    "    match = re.search(r\"<answer>(.*?)</answer>\", answer, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return answer\n",
    "\n",
    "def calculate_format_reward(response):\n",
    "    # required tags\n",
    "    required = [\"<think>\", \"</think>\", \"<answer>\", \"</answer>\"]\n",
    "    if any(tag not in response for tag in required):\n",
    "        return -0.5\n",
    "\n",
    "    think_open = response.find(\"<think>\")\n",
    "    think_close = response.find(\"</think>\")\n",
    "    answer_open = response.find(\"<answer>\")\n",
    "    answer_close = response.find(\"</answer>\")\n",
    "\n",
    "    # enforce correct order\n",
    "    if not (0 <= think_open < think_close < answer_open < answer_close):\n",
    "        return -0.5\n",
    "\n",
    "    reward = 0.0\n",
    "    reward += 0.2  # correct format\n",
    "    reward += 0.3  # answer block exists\n",
    "    return reward\n",
    "\n",
    "def correctness_reward(response, validation_object):\n",
    "    # This might fail if the dataset requires specific logic, wrapping in try/except is safer\n",
    "    try:\n",
    "        score_fn = get_score_answer_fn(validation_object['metadata']['source_dataset'])\n",
    "        return score_fn(response, validation_object)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def calculate_rewards(batch_response, validation_objects):\n",
    "    format_r = np.array([calculate_format_reward(r) for r in batch_response])\n",
    "    correctness_r = np.array([\n",
    "        correctness_reward(extract_answer(r), val_obj) \n",
    "        for val_obj, r in zip(validation_objects, batch_response)\n",
    "    ])\n",
    "    return FORMAT_REWARD_WEIGHT * format_r + CORRECTNESS_REWARD_WEIGHT * correctness_r\n",
    "\n",
    "# --- Model & Math Utils ---\n",
    "\n",
    "def calculate_logits(model, input_ids, attention_mask):\n",
    "    \"\"\"\n",
    "    Calculates token-level log probabilities for the specific tokens in input_ids.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits[:, :-1, :]  # Shift so we predict next token\n",
    "        input_ids_shifted = input_ids[:, 1:]\n",
    "        \n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        token_logprobs = torch.gather(log_probs, 2, input_ids_shifted.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        return token_logprobs\n",
    "\n",
    "def left_pad(sequences, pad_value):\n",
    "    \"\"\"Left-pads a list of 1D tensors.\"\"\"\n",
    "    max_len = max(len(s) for s in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    dtype = sequences[0].dtype\n",
    "    device = sequences[0].device\n",
    "    \n",
    "    padded_batch = torch.full((batch_size, max_len), fill_value=pad_value, dtype=dtype, device=device)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_batch[i, -len(seq):] = seq\n",
    "        \n",
    "    return padded_batch\n",
    "\n",
    "# --- Data Loading ---\n",
    "\n",
    "class GymDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "    prompts = []\n",
    "    for sample in batch:\n",
    "        question = sample['question']\n",
    "        prompt = (\n",
    "            f\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\\n\"\n",
    "            f\"The assistant first thinks about the reasoning process in the mind and then provides the user \"\n",
    "            f\"with the answer.\\nUser: {question}.\\nAssistant:\"\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    # Basic tokenization of prompts\n",
    "    input_tokenized = tokenizer(prompts, padding=True, add_special_tokens=False, return_tensors='pt')\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_tokenized['input_ids'],\n",
    "        'attention_mask': input_tokenized['attention_mask'],\n",
    "        'raw_batch': batch  # Keep original objects for validation\n",
    "    }\n",
    "\n",
    "def get_dataloader(dataset_name, tokenizer, batch_size=2):\n",
    "    # Note: 'syllogism' might need to be 'logic/syllogism' or similar depending on reasoning_gym version\n",
    "    gym_data = reasoning_gym.create_dataset(dataset_name, seed=42, size=100) \n",
    "    data = GymDataset(gym_data)\n",
    "    return DataLoader(\n",
    "        data,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=partial(collate_fn, tokenizer=tokenizer),\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c33eb884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_exp(model, tokenizer, input_ids, raw_batch, attention_mask):\n",
    "    model.eval() # Ensure eval mode for generation\n",
    "    \n",
    "    # 1. Generate Rollouts\n",
    "    with torch.no_grad():\n",
    "        # input_ids shape: [B, Seq]\n",
    "        # full_response shape: [B * N_ROLLOUTS, Total_Seq]\n",
    "        full_response = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            temperature=1.0,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=N_ROLLOUTS,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        # 2. Separate Prompt and Completion\n",
    "        # Note: full_response includes input_ids. We need to find where generation started.\n",
    "        prompt_len = input_ids.shape[1]\n",
    "        completion_ids = full_response[:, prompt_len:]\n",
    "        \n",
    "        # 3. Calculate Log Probs of the Generated Sequence (Reference Policy)\n",
    "        # We need full attention mask for the generated sequence\n",
    "        full_attention_mask = (full_response != tokenizer.pad_token_id).long()\n",
    "        \n",
    "        # This returns log_probs for tokens at indices [1:] (next token prediction)\n",
    "        # Shape: [B*N, Total_Seq-1]\n",
    "        all_token_log_probs = calculate_logits(model, full_response, full_attention_mask)\n",
    "        \n",
    "        # Extract only the completion log probs\n",
    "        # We align by slicing. all_token_log_probs index i corresponds to prediction of full_response[i+1]\n",
    "        # The first token of completion is at prompt_len. It was predicted by token at prompt_len-1.\n",
    "        completion_log_probs = all_token_log_probs[:, prompt_len-1:]\n",
    "\n",
    "        # 4. Decoding & Rewards\n",
    "        decoded_responses = tokenizer.batch_decode(completion_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Repeat validation objects for the rollouts\n",
    "        # raw_batch is size B, we need B*N\n",
    "        expanded_val_objs = []\n",
    "        for obj in raw_batch:\n",
    "            expanded_val_objs.extend([obj] * N_ROLLOUTS)\n",
    "\n",
    "        rewards = calculate_rewards(decoded_responses, expanded_val_objs)\n",
    "        \n",
    "        # 5. Advantage Calculation (Group Relative)\n",
    "        # Reshape to [B, N] to normalize within the group\n",
    "        rewards_reshaped = rewards.reshape(-1, N_ROLLOUTS)\n",
    "        mean_rewards = rewards_reshaped.mean(axis=1, keepdims=True)\n",
    "        std_rewards = rewards_reshaped.std(axis=1, keepdims=True) + 1e-8\n",
    "        advantages = (rewards_reshaped - mean_rewards) / std_rewards\n",
    "        \n",
    "        # Flatten back to [B*N] and move to tensor\n",
    "        advantages = torch.tensor(advantages.flatten(), dtype=torch.float32, device=accelerator.device)\n",
    "\n",
    "        experiences = []\n",
    "        for i in range(len(full_response)):\n",
    "            # Create a mask for ONLY the completion part (ignoring padding)\n",
    "            # 1 for valid completion tokens, 0 for everything else\n",
    "            seq_len = full_response[i].size(0)\n",
    "            mask = torch.zeros(seq_len, dtype=torch.float32, device=accelerator.device)\n",
    "            \n",
    "            # Find end of valid tokens (first pad after prompt) or end of seq\n",
    "            is_pad = (full_response[i] == tokenizer.pad_token_id)\n",
    "            # We only care about padding occurring AFTER the prompt\n",
    "            completion_pads = is_pad[prompt_len:]\n",
    "            if completion_pads.any():\n",
    "                # First pad index relative to start of completion\n",
    "                valid_len = completion_pads.nonzero(as_tuple=True)[0][0].item()\n",
    "                end_idx = prompt_len + valid_len\n",
    "            else:\n",
    "                end_idx = seq_len\n",
    "                \n",
    "            mask[prompt_len:end_idx] = 1.0\n",
    "\n",
    "            # Store the log probs needed for optimization (shifted internally in loss calc, but here we store raw)\n",
    "            # Actually, to save memory, we usually just recompute logits during training or store current log_probs.\n",
    "            # Storing `completion_log_probs` requires alignment. \n",
    "            # Let's store the Full Sequence and recompute/extract in training loop for simplicity or alignment.\n",
    "            \n",
    "            experiences.append({\n",
    "                'input_ids': full_response[i], # [Seq]\n",
    "                'attention_mask': full_attention_mask[i], # [Seq]\n",
    "                'response_mask': mask, # [Seq] (1s on completion, 0s elsewhere)\n",
    "                'old_log_probs': all_token_log_probs[i], # [Seq-1]\n",
    "                'advantages': advantages[i] # Scalar\n",
    "            })\n",
    "            \n",
    "    return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6250b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grpo_loss(new_log_probs, old_log_probs, response_mask, advantages):\n",
    "    \"\"\"\n",
    "    new_log_probs: [B, Seq-1]\n",
    "    old_log_probs: [B, Seq-1]\n",
    "    response_mask: [B, Seq] -> Needs to be shifted to [B, Seq-1] to match logits\n",
    "    advantages: [B, 1]\n",
    "    \"\"\"\n",
    "    # Align mask with logits (drop the first token position as logits predict next token)\n",
    "    mask = response_mask[:, 1:]\n",
    "    \n",
    "    # Probability Ratio\n",
    "    # exp(new - old) = new_prob / old_prob\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    \n",
    "    # Clip\n",
    "    clip_epsilon = 0.2\n",
    "    clipped_ratio = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon)\n",
    "    \n",
    "    # Element-wise loss\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = clipped_ratio * advantages\n",
    "    min_surr = torch.min(surr1, surr2)\n",
    "    \n",
    "    # Mask out non-response tokens\n",
    "    masked_loss = min_surr * mask\n",
    "    \n",
    "    # Average over valid response tokens only\n",
    "    loss = -masked_loss.sum() / (mask.sum() + 1e-8)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train_step(model, batch, optimizer):\n",
    "    model.train()\n",
    "    \n",
    "    # 1. Pad Batch\n",
    "    input_ids = left_pad([b['input_ids'] for b in batch], tokenizer.pad_token_id)\n",
    "    attention_mask = left_pad([b['attention_mask'] for b in batch], 0)\n",
    "    response_mask = left_pad([b['response_mask'] for b in batch], 0)\n",
    "    \n",
    "    # Old log probs need to be padded. They are usually float. Pad with 0.\n",
    "    old_log_probs = left_pad([b['old_log_probs'] for b in batch], 0.0)\n",
    "    \n",
    "    advantages = torch.stack([b['advantages'] for b in batch]).unsqueeze(-1) # [B, 1]\n",
    "    \n",
    "    # 2. Forward Pass (Recompute Logits)\n",
    "    # This calls model() so gradients flow\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits[:, :-1, :]\n",
    "    input_ids_shifted = input_ids[:, 1:]\n",
    "    \n",
    "    log_probs_all = torch.log_softmax(logits, dim=-1)\n",
    "    new_token_log_probs = torch.gather(log_probs_all, 2, input_ids_shifted.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # 3. Calculate Loss\n",
    "    loss = calculate_grpo_loss(\n",
    "        new_log_probs=new_token_log_probs,\n",
    "        old_log_probs=old_log_probs,\n",
    "        response_mask=response_mask,\n",
    "        advantages=advantages\n",
    "    )\n",
    "    \n",
    "    # 4. Backward\n",
    "    optimizer.zero_grad()\n",
    "    accelerator.backward(loss)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1500fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, buffer, optimizer, num_epochs=2):\n",
    "    random.shuffle(buffer)\n",
    "    losses = []\n",
    "    \n",
    "    # TQDM for training epochs\n",
    "    pbar = tqdm(range(num_epochs), desc=\"Training Epochs\", leave=False)\n",
    "    for _ in pbar:\n",
    "        epoch_losses = []\n",
    "        for i in range(0, len(buffer), BATCH_SIZE):\n",
    "            batch = buffer[i : i + BATCH_SIZE]\n",
    "            if len(batch) == 0: continue\n",
    "            \n",
    "            loss_val = train_step(model, batch, optimizer)\n",
    "            epoch_losses.append(loss_val)\n",
    "            \n",
    "        avg_loss = np.mean(epoch_losses) if epoch_losses else 0\n",
    "        losses.append(avg_loss)\n",
    "        pbar.set_postfix({\"Avg Loss\": f\"{avg_loss:.4f}\"})\n",
    "        \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "94bbd2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Loading Model...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mLoading Model\u001b[0m\u001b[1;32m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Starting Collection &amp; Training...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mStarting Collection & Training\u001b[0m\u001b[1;32m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data Collection:   0%|          | 0/50 [01:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[197]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     38\u001b[39m main_pbar = tqdm(data_loader, desc=\u001b[33m\"\u001b[39m\u001b[33mData Collection\u001b[39m\u001b[33m\"\u001b[39m, disable=\u001b[38;5;129;01mnot\u001b[39;00m accelerator.is_local_main_process)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(main_pbar):\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# Collect Experience\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     experiences = \u001b[43mcollect_exp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mraw_batch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     buffer.extend(experiences)\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# Trigger training when buffer is full\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[193]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mcollect_exp\u001b[39m\u001b[34m(model, tokenizer, input_ids, raw_batch, attention_mask)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 1. Generate Rollouts\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# input_ids shape: [B, Seq]\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# full_response shape: [B * N_ROLLOUTS, Total_Seq]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     full_response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_NEW_TOKENS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_ROLLOUTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# 2. Separate Prompt and Completion\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Note: full_response includes input_ids. We need to find where generation started.\u001b[39;00m\n\u001b[32m     21\u001b[39m     prompt_len = input_ids.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/basicenv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/basicenv/lib/python3.13/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/basicenv/lib/python3.13/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/basicenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/basicenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/basicenv/lib/python3.13/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/basicenv/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:473\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[32m    472\u001b[39m slice_indices = \u001b[38;5;28mslice\u001b[39m(-logits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/basicenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/basicenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/basicenv/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load Tokenizer\n",
    "    base_model_id = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    tokenizer.padding_side = \"left\" # Important for generation\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    # Load Model\n",
    "    print(\"[bold green]Loading Model...[/bold green]\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id, \n",
    "        torch_dtype=torch.bfloat16,\n",
    "        # device_map=\"auto\" # Let Accelerator handle device\n",
    "    )\n",
    "    \n",
    "    # Load PEFT adapter (Update path as needed)\n",
    "    model = PeftModel.from_pretrained(base_model, '/Users/peeyushsharma/Downloads/lora_sft_2')\n",
    "    # For demonstration, we just use the base model as if it were the peft model\n",
    "    model = base_model \n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    \n",
    "    # Data Loader\n",
    "    data_loader = get_dataloader('syllogism', tokenizer)\n",
    "    \n",
    "    # Prepare with Accelerator\n",
    "    # Note: If using PeftModel, ensure only adapter params are optimized\n",
    "    model, optimizer, data_loader = accelerator.prepare(model, optimizer, data_loader)\n",
    "    \n",
    "    buffer = []\n",
    "    \n",
    "    # Main Loop\n",
    "    print(\"[bold green]Starting Collection & Training...[/bold green]\")\n",
    "    \n",
    "    # TQDM over the dataloader\n",
    "    main_pbar = tqdm(data_loader, desc=\"Data Collection\", disable=not accelerator.is_local_main_process)\n",
    "    \n",
    "    for i, sample in enumerate(main_pbar):\n",
    "        # Collect Experience\n",
    "        experiences = collect_exp(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            sample['input_ids'], \n",
    "            sample['raw_batch'], \n",
    "            sample['attention_mask']\n",
    "        )\n",
    "        buffer.extend(experiences)\n",
    "        \n",
    "        # Trigger training when buffer is full\n",
    "        if len(buffer) >= BUFFER_SIZE:\n",
    "            main_pbar.set_description(\"Training...\")\n",
    "            avg_loss = train_loop(model, buffer, optimizer, num_epochs=EPOCHS)\n",
    "            \n",
    "            # Clear buffer and update stats\n",
    "            buffer = []\n",
    "            main_pbar.set_description(f\"Collecting (Last Loss: {avg_loss:.4f})\")\n",
    "            \n",
    "    print(\"[bold green]Finished![/bold green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec8384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basicenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
